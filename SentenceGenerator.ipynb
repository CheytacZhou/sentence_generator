{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"SentenceGenerator.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNABisfFW9TsF/pqXUf20vs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gsj2bbc56oA1","executionInfo":{"status":"ok","timestamp":1621907938927,"user_tz":-480,"elapsed":16352,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}},"outputId":"9d94a26a-0d44-486d-83ca-75023da817d1"},"source":["import os\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"R1ZuZW6V6qiE","executionInfo":{"status":"ok","timestamp":1621907940882,"user_tz":-480,"elapsed":4,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}}},"source":["### Change working directory\n","\n","path = '/content/gdrive/My Drive/NLG_2021_05_21'\n","os.chdir(path)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"F8i2bQTgcwc3","executionInfo":{"status":"ok","timestamp":1621907952206,"user_tz":-480,"elapsed":3175,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}}},"source":["import os\n","import csv\n","import jieba\n","import json\n","import numpy as np\n","from pkg_resources import resource_filename\n","from keras.preprocessing import sequence\n","from sklearn.preprocessing import LabelBinarizer\n","\n","class PreProcessor():\n","\n","  config = {\n","      \"meta_token\": \"<s>\",\n","      \"max_length\": 100,\n","      \"max_words\": 50000\n","  }\n","\n","  def __init__(self, vocab_path = None, config_path = None, name = \"TextGenerator\"):\n","    self.vocab = None\n","    self.config.update({\"name\": name})\n","    if vocab_path is None:\n","      vocab_path = resource_filename(__name__, \"{}_vocab.json\".format(self.config[\"name\"]))\n","    if config_path is not None:\n","      with open(config_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as json_file:\n","        self.config = json.load(json_file)\n","    if os.path.exists(vocab_path):\n","      with open(vocab_path, \"r\", errors = \"ignore\") as json_file:\n","        self.vocab = json.load(json_file)\n","        self.num_classes = len(self.vocab) + 1\n","        self.config.update({\"num_classes\": self.num_classes})\n","        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","  \n","  def read_textfile(self, file_path, context = None, header = True, is_csv = False):\n","    \"\"\" Retrieves texts from a newline-delimited file and returns as a list. \"\"\"\n","    self.config.update({\"context\": context})\n","    if context:\n","      texts = []\n","      contexts = []\n","    else:\n","      texts = []\n","    if is_csv:\n","      with open(file_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as f:\n","        reader = csv.reader(f)\n","        for row in reader:\n","          if context:\n","            try:\n","              texts.append(row[1])\n","              contexts.append(row[0])\n","            except Exception:\n","              pass \n","          else:\n","            try:\n","              texts.append(row[1])\n","            except Exception:\n","              pass\n","    else:\n","      if header:\n","        with open(file_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as f:\n","          f.readline()\n","          for line in f:\n","            row = line.split()\n","            if context:\n","              texts.append(row[0])\n","              contexts.append(row[1])\n","            else:\n","              texts.append(row[0])\n","    for i in range(len(texts)):\n","      texts[i] = self.regex_process(texts[i])\n","      texts[i] = jieba.lcut(texts[i])\n","    self.word_count(texts)\n","    if context:\n","      return texts, contexts\n","    else:\n","      return texts\n","  \n","  def regex_process(self, texts):\n","    import re\n","    from string import punctuation\n","    texts = re.sub(r\"[br<br/>\\n]\", \"\", texts)\n","    texts = re.sub(r\"[1-9]+[(，、.)]\", \"\", texts)\n","    texts = re.sub(r\"【\\w+】\", \"\", texts)\n","    texts = re.sub(r\"】\", \"\", texts)\n","    punc = punctuation + u\"。！？＂＃＄％＆＇（）＊＋，－／：；＜＝＞＠［＼］＾＿｀｛｜｝～｟｠｢｣､、〃》「」『』【】〔〕〖〗〘〙〚〛〜〝〞〟〰〾〿–—‘’‛“”„‟…‧﹏.·\"\n","    texts = re.sub(r\"[{}]+\".format(punc), \" \", texts)\n","    return texts\n","\n","  def word_count(self, texts):\n","    self.wordcount = {}\n","    for i in range(len(texts)):\n","      for word in texts[i]:\n","        if word in self.wordcount:\n","          self.wordcount[word] += 1\n","        else:\n","          self.wordcount[word] = 1\n","    count_list = sorted([(v, k) for k, v in self.wordcount.items()], reverse = True)\n","    self.wordcount = {v: k for i, (k, v) in enumerate(count_list)}\n","\n","  def generate_vocabulary(self, texts):\n","    \"\"\" Create a vocabulary in the form of \"text: index\" for training set. \"\"\"\n","    if self.vocab is None:\n","      vocab = []\n","    else:\n","      vocab = list(self.vocab.keys())\n","    for i in range(len(texts)):\n","      vocab.extend(word for word in texts[i])\n","    vocab = set(vocab)\n","    # Limit vocab to max_words\n","    self.vocab = {}\n","    for i, t in enumerate(vocab):\n","      if i <= self.config[\"max_words\"]:\n","        self.vocab[t] = i + 1\n","    if self.config[\"meta_token\"] not in self.vocab.keys():\n","      self.vocab[self.config[\"meta_token\"]] = len(self.vocab) + 1\n","    self.num_classes = len(self.vocab) + 1\n","    self.config.update({\"num_classes\": self.num_classes})\n","    self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","    # save the files needed to recreate the model\n","    with open(\"{}_vocab.json\".format(self.config[\"name\"]), \"w\", encoding = \"utf-8\") as outfile:\n","      json.dump(self.vocab, outfile, ensure_ascii = False)\n","\n","  def process_sequence(self, X):\n","    \"\"\" Padding the sequence in order to make sure all the sequence in a fixed length. \"\"\"\n","    X = [[self.vocab[w] for w in X[0]]]\n","    X = sequence.pad_sequences(X, maxlen = self.config[\"max_length\"])\n","    return X\n","\n","  def encode_cat(self, chars, vocab):\n","    \"\"\" One hot encodes values at given chars efficiently by preallocating a zeros matrix. \"\"\"\n","    a = np.float32(np.zeros((len(chars), len(vocab) + 1)))\n","    rows, cols = zip(*[(i, vocab.get(char, 0)) for i, char in enumerate(chars)])\n","    a[rows, cols] = 1\n","    return a\n","\n","  def generate_sequences_from_texts(self, texts, indices_list, contexts = None, batch_size = 128):\n","    \"\"\" Generate the training batch from text. \"\"\"\n","    while True:\n","      np.random.shuffle(indices_list)\n","      X_batch = []\n","      Y_batch = []\n","      context_batch = []\n","      count_batch = 0\n","      for row in range(indices_list.shape[0]):\n","        text_index = indices_list[row, 0]\n","        end_index = indices_list[row, 1]\n","        text = texts[text_index]\n","        text = [self.config[\"meta_token\"]] + text + [self.config[\"meta_token\"]]\n","        if end_index > self.config[\"max_length\"]:\n","          x = text[end_index - self.config[\"max_length\"] : end_index + 1]\n","        else:\n","          x = text[0 : end_index + 1]\n","        y = text[end_index + 1]\n","        if y in self.vocab:\n","          x = self.process_sequence([x])\n","          y = self.encode_cat([y], self.vocab)\n","          X_batch.append(x)\n","          Y_batch.append(y)\n","          if contexts is not None:\n","            context_batch.append(contexts[text_index])\n","          count_batch += 1\n","          if count_batch % batch_size == 0:\n","            X_batch = np.squeeze(np.array(X_batch))\n","            Y_batch = np.squeeze(np.array(Y_batch))\n","            context_batch = np.squeeze(np.array(context_batch))\n","            if contexts is not None:\n","              yield ([X_batch, context_batch], [Y_batch, Y_batch])\n","            else:\n","              yield (X_batch, Y_batch)\n","            X_batch = []\n","            Y_batch = []\n","            context_batch = []\n","            count_batch = 0\n","\n","  def generate_generator(self, texts, contexts = None, train_size = 0.7, validation = True, batch_size = 128):\n","    \"\"\" Generate the generator for model training. \"\"\"\n","    gen = None\n","    gen_val = None\n","    self.val_steps = None\n","    if contexts:\n","      contexts = LabelBinarizer().fit_transform(contexts)\n","      self.config.update({\"context_size\": contexts.shape[1]})\n","    # calculate all combinations of text indices + token indices\n","    indices_list = np.block([np.meshgrid(np.array(i), np.arange(len(text) + 1)) for i, text in enumerate(texts)])\n","    indices_mask = np.random.rand(indices_list.shape[0]) < train_size\n","    indices_list_train = indices_list[indices_mask, :]\n","    num_tokens = indices_list_train.shape[0]\n","    assert num_tokens >= batch_size, \"Fewer tokens than batch_size\"\n","    self.steps_per_epoch = max(int(np.floor(num_tokens / batch_size)), 1)\n","    self.config.update({\"steps_per_epoch\": self.steps_per_epoch})\n","    gen = self.generate_sequences_from_texts(texts = texts, contexts = contexts, indices_list = indices_list, batch_size = batch_size)\n","    # generate text indices for validaiton  \n","    if train_size < 1.0 and validation:\n","      indices_list_val = indices_list[~indices_mask, :]\n","      gen_val = self.generate_sequences_from_texts(texts = texts, contexts = contexts, indices_list = indices_list_val, batch_size = batch_size)\n","      self.val_steps = max(int(np.floor(indices_list_val.shape[0] / batch_size)), 1)\n","      self.config.update({\"val_steps\": self.val_steps})\n","    return gen, gen_val\n","  \n","  def main(self, file_path, context, is_csv, train_size, validation, batch_size):\n","    if context:\n","      texts, contexts = self.read_textfile(file_path, context, is_csv = is_csv)\n","    else:\n","      texts = self.read_textfile(file_path, context, is_csv = is_csv)\n","      contexts = None\n","    self.word_count(texts)\n","    self.generate_vocabulary(texts)\n","    gen, gen_val = proc.generate_generator(texts, contexts, train_size = train_size, validation = validation, batch_size = batch_size)\n","    self.config.update({\"train_size\": train_size})\n","    self.config.update({\"validaiton\": validation})\n","    self.config.update({\"batch_size\": batch_size})\n","    with open('{}_config.json'.format(self.config['name']), 'w', encoding='utf8') as outfile:\n","      json.dump(self.config, outfile, ensure_ascii=False)\n","    return gen, gen_val"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TkuMnWWZsatW","executionInfo":{"status":"ok","timestamp":1621588426366,"user_tz":-480,"elapsed":19143,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}},"outputId":"4f3c6825-8db7-48d5-b637-28cdcdb3e23e"},"source":["proc = PreProcessor()\n","gen, gen_val = proc.main(\"large_text.csv\", context = True, is_csv = True, train_size = 0.7, validation = True, batch_size = 128)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Building prefix dict from the default dictionary ...\n","Dumping model to file cache /tmp/jieba.cache\n","Loading model cost 1.040 seconds.\n","Prefix dict has been built successfully.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gkqi3hOvkKDW","executionInfo":{"status":"ok","timestamp":1621908891383,"user_tz":-480,"elapsed":1221,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}}},"source":["from keras.callbacks import LearningRateScheduler, Callback\n","from keras.models import Model, load_model\n","from keras.optimizers import RMSprop\n","from keras import backend as K\n","from pkg_resources import resource_filename\n","from keras.engine import InputSpec, Layer\n","from keras import initializers\n","from keras.layers import Input, Embedding, Dense, LSTM, Bidirectional\n","from keras.layers import concatenate, Reshape, SpatialDropout1D\n","from keras.preprocessing import sequence\n","from random import shuffle\n","from pkg_resources import resource_filename\n","from tqdm import trange\n","\n","import os\n","import numpy as np\n","import json\n","\n","class TextGenerator:\n","\n","  def __init__(self, weights_path = None, vocab_path = None, config_path = None, name = \"TextGenerator\"):\n","    if config_path is not None:\n","      with open(config_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as json_file:\n","        self.config = json.load(json_file)\n","    self.config.update({\"name\": name})\n","    self.config.update({\"rnn_layers\": 2})\n","    self.config.update({\"rnn_size\": 256})\n","    self.config.update({\"rnn_bidirectional\": True})\n","    self.config.update({\"dim_embeddings\": 300})\n","    if weights_path is None:\n","      weights_path = resource_filename(__name__, \"{}_weights.hdf5\".format(self.config[\"name\"]))\n","    if vocab_path is None:\n","      vocab_path = resource_filename(__name__, \"{}_vocab.json\".format(self.config[\"name\"]))\n","    if os.path.exists(vocab_path) is not None:\n","      with open(vocab_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as json_file:\n","        self.vocab = json.load(json_file)\n","        self.num_classes = len(self.vocab) + 1\n","        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","    if os.path.exists(weights_path) is not None:\n","      self.model = self.build_textgen_model(weights_path = weights_path)\n","    else:\n","      self.model = None\n","  \n","  def build_textgen_model(self, context_size = None, weights_path = None, dropout = 0.0, optimizer = RMSprop(lr = 3e-4, rho = 0.99)):\n","    \"\"\" Builds the model architecture for textgen and loads the specified weights for the model. \"\"\"\n","    # build rnn layer\n","    def new_rnn(layer_num):\n","      if self.config[\"rnn_bidirectional\"]:\n","        return Bidirectional(LSTM(self.config[\"rnn_size\"], return_sequences = True, recurrent_activation = \"sigmoid\"), name = \"rnn_{}\".format(layer_num))\n","      return LSTM(self.config[\"rnn_size\"], return_sequences = True, recurrent_activation = \"sigmoid\", name = \"rnn_{}\".format(layer_num))\n","    # input layer\n","    input = Input(shape = (self.config[\"max_length\"], ), name = \"input\")\n","    # embedding layer\n","    embedding = Embedding(self.num_classes, self.config[\"dim_embeddings\"], input_length = self.config[\"max_length\"], name = \"embedding\")(input)\n","    # dropout\n","    self.config.update({\"dropout\": dropout})\n","    if dropout > 0.0:\n","      embedding = SpatialDropout1D(dropout, name = \"dropout\")(embedding)\n","    # rnn layer\n","    rnn_layer_list = []\n","    for i in range(self.config[\"rnn_layers\"]):\n","      prev_layer = embedding if i is 0 else rnn_layer_list[-1]\n","      rnn_layer_list.append(new_rnn(i+1)(prev_layer))\n","    # concatenation\n","    seq_concat = concatenate([embedding] + rnn_layer_list, name = \"rnn_concat\")\n","    # attention layer\n","    attention = AttentionWeightedAverage(name = \"attention\")(seq_concat)\n","    # output layer\n","    output = Dense(self.num_classes, name = \"output\", activation = \"softmax\")(attention)\n","    # whether to include the context\n","    if context_size is None:\n","      model = Model(inputs = [input], outputs = [output])\n","      if weights_path is not None:\n","        model.load_weights(weights_path, by_name = True)\n","      model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer)\n","    else:\n","      context_input = Input(shape = (context_size, ), name = \"context_input\")\n","      context_reshape = Reshape((context_size, ), name = \"context_reshape\")(context_input)\n","      merged = concatenate([attention, context_reshape], name = \"concat\")\n","      main_output = Dense(self.num_classes, name = \"context_output\", activation = \"softmax\")(merged)\n","      model = Model(inputs = [input, context_input], outputs = [main_output, output])\n","      if weights_path is not None:\n","        model.load_weights(weights_path, by_name = True)\n","      model.compile(loss = \"categorical_crossentropy\", optimizer = optimizer, loss_weights = [0.8, 0.2])\n","    return model\n","\n","  def train_textgen(self, gen, gen_val, dropout, num_epochs, gen_epochs, save_epochs, verbose, context = None):\n","    \"\"\" Train the model built before. \"\"\"\n","    base_lr = 3e-4\n","    self.config.update({\"base_lr\": base_lr})\n","    # scheduler function must be defined inline\n","    def lr_linear_decay(epoch):\n","      return (base_lr * (1 - (epoch / num_epochs)))\n","    if self.model is None:\n","      if context:\n","        self.model = self.build_textgen_model(dropout = dropout, context_size = self.config[\"context_size\"])\n","      else:\n","        self.model = self.build_textgen_model(dropout = dropout)\n","    with open('{}_config.json'.format(self.config['name']), 'w', encoding='utf8') as outfile:\n","      json.dump(self.config, outfile, ensure_ascii=False)\n","    self.model.fit(\n","        gen, \n","        steps_per_epoch = self.config[\"steps_per_epoch\"], \n","        epochs = num_epochs, \n","        callbacks = [\n","          LearningRateScheduler(lr_linear_decay), \n","          generate_after_epoch(self, gen_epochs, self.config[\"max_length\"]), \n","          save_model_weights(self, num_epochs, save_epochs)\n","        ],\n","        verbose = verbose,\n","        max_queue_size = 10,\n","        validation_data = gen_val,\n","        validation_steps = self.config[\"val_steps\"]\n","    )\n","\n","  def generate(self, n = 1, return_as_list = False, prefix = None, temperature = [1.0, 0.5, 0.2, 0.2], interactive = False, top_n = 5, progress = True):\n","    gen_texts = []\n","    iterable = trange(n) if progress and n > 1 else range(n)\n","    for _ in iterable:\n","      gen_text, _ = self.generate_text(\n","          self.config[\"max_length\"],\n","          top_n,\n","          temperature, \n","          interactive, \n","          prefix\n","      )\n","      if not return_as_list:\n","        print(\"{}\\n\".format(gen_text))\n","      gen_texts.append(gen_text)\n","    if return_as_list:\n","      return gen_texts\n","  \n","  def generate_samples(self, n = 1, temperatures = [0.0, 0.2, 0.5, 1.0], **kwargs):\n","    for temperature in temperatures:\n","      print(\"#\" * 20 + \"\\nTemperature: {}\\n\".format(temperature) + \"#\" * 20)\n","      self.generate(n, temperature = temperature, progress = False, **kwargs)\n","      \n","  def generate_text(self, maxlen, top_n, temperature = 0.5, interactive = False, prefix = None, synthesize = False, stop_tokens = [\" \", \"\\n\"]):\n","    \"\"\" Generates and returns a single text. \"\"\"\n","    collapse_char = \"\"\n","    end = False\n","    text = [self.config[\"meta_token\"]] + jieba.lcut(prefix) if prefix else [self.config[\"meta_token\"]]\n","    next_char = \"\"\n","    if not isinstance(temperature, list):\n","      temperature = [temperature]\n","    if len(self.model.inputs) > 1:\n","      self.model_gen = Model(inputs = self.model.inputs[0], outputs = self.model.outputs[1])\n","    else:\n","      self.model_gen = self.model\n","    while not end and len(text) < self.config[\"max_length\"]:\n","      encoded_text = self.prediction_encode_sequence(text[-maxlen:], self.vocab, maxlen)\n","      next_temperature = temperature[(len(text) - 1) % len(temperature)]\n","      if not interactive:\n","        # auto-generate text without user intervention\n","        next_index = self.generate_next_char(self.model_gen.predict(encoded_text, batch_size = 1)[0], next_temperature)\n","        next_char = self.indices_char[next_index]\n","        text += [next_char]\n","        if next_char == self.config[\"meta_token\"] or len(text) >= self.config[\"max_length\"]:\n","          end = True\n","        gen_break = (next_char in stop_tokens or len(stop_tokens) == 0)\n","        if synthesize and gen_break:\n","          break\n","      else:\n","        # ask user what the next char/word should be\n","        options_index = self.generate_sample(self.model_gen.predict(encoded_text, batch_size = 1)[0], next_temperature, interactive = interactive, top_n = top_n)\n","        options = [self.indices_char[idx] for idx in options_index]\n","        print(\"Controls:\\n\\ts: stop.\\tx: backspace.\\to: write your own.\")\n","        print(\"\\nOptions:\")\n","        for i, option in enumerate(options, 1):\n","          print(\"\\t{}: {}\".format(i, option))\n","        print(\"\\nProgress: {}\".format(collapse_char.join(text)[3:]))\n","        print(\"\\nYour choice?\")\n","        user_input = input(\"> \")\n","        try:\n","          user_input = int(user_input)\n","          next_char = options[user_input - 1]\n","          text += [next_char]\n","          if next_char == \"<s>\":\n","            end = True\n","        except ValueError:\n","          if user_input == \"s\":\n","            next_char = \"<s>\"\n","            text += [next_char]\n","          elif user_input == \"o\":\n","            other = input(\"> \")\n","            text += [other]\n","          elif user_input == \"x\":\n","            try:\n","              del text[-1]\n","            except IndexError:\n","              pass\n","          else:\n","            print(\"That\\'s not an option!\")\n","    # if not single text, remove the <s> meta_tokens\n","    text = text[1:]\n","    if self.config[\"meta_token\"] in text:\n","      text.remove(self.config[\"meta_token\"])\n","    text_joined = collapse_char.join(text)\n","    return text_joined, end\n","\n","  def prediction_encode_sequence(self, text, vocab, maxlen):\n","    \"\"\" Encodes a text into the corresponding encoding for prediction with the model. \"\"\"\n","    encoded = np.array([vocab.get(x, 0) for x in text])\n","    return sequence.pad_sequences([encoded], maxlen = maxlen)\n","  \n","  def generate_next_char(self, preds, temperature, interactive = False, top_n = 5):\n","    \"\"\" Samples predicted probabilities of the next character to allow for the network to show \"creativity\". \"\"\"\n","    preds = np.asarray(preds).astype(\"float64\")\n","    if temperature is None or temperature == 0.0:\n","      return np.argmax(preds)\n","    preds = np.log(preds + K.epsilon()) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    if not interactive:\n","      index = np.argmax(probas)\n","      # prevent function from being able to choose 0 (placeholder)\n","      # choose 2nd best index from preds\n","      if index == 0:\n","        index = np.argsort(preds)[-2]\n","    else:\n","      # return list of top N chars/words descending order, based on probability\n","      index = (-preds).argsort()[:top_n]\n","    return index\n","\n","class generate_after_epoch(Callback):\n","  def __init__(self, textgen, gen_epochs, max_gen_length):\n","    self.textgen = textgen\n","    self.gen_epochs = gen_epochs\n","    self.max_gen_length = max_gen_length\n","  \n","  def on_epoch_end(self, epoch, logs = {}):\n","    if self.gen_epochs > 0 and (epoch + 1) % self.gen_epochs == 0:\n","      self.textgen.generate_samples()\n","\n","class save_model_weights(Callback):\n","  def __init__(self, textgen, num_epochs, save_epochs):\n","    self.textgen = textgen\n","    self.weights_name = textgen.config[\"name\"]\n","    self.num_epochs = num_epochs\n","    self.save_epochs = save_epochs\n","  \n","  def on_epoch_end(self, epoch, log = {}):\n","    if len(self.textgen.model.inputs) > 1:\n","      self.textgen.model = Model(inputs = self.model.input[0], outputs = self.model.output[1])\n","    if self.save_epochs > 0 and (epoch + 1) % self.save_epochs == 0 and self.num_epochs != (epoch + 1):\n","      print(\"Saving Model Weights - Epoch #{}\".format(epoch + 1))\n","      self.textgen.model.save_weights(\"{}_weights_epoch_{}.hdf5\".format(self.weights_name, epoch + 1))\n","    else:\n","      self.textgen.model.save_weights(\"{}_weights.hdf5\".format(self.weights_name))\n","\n","class AttentionWeightedAverage(Layer):\n","  \"\"\" This attention layer code is from DeepMoji (MIT Licensed). \"\"\"\n","  \"\"\" Computes a weighted average of the different channels across timesteps. Users 1 parameter pr. channel to compute the attention value for a single timestep. \"\"\"\n","  def __init__(self, return_attention = False, **kwargs):\n","    self.init = initializers.get(\"uniform\")\n","    self.supports_masking = True\n","    self.return_attention = return_attention\n","    super(AttentionWeightedAverage, self).__init__(**kwargs)\n","  \n","  def build(self, input_shape):\n","    self.input_spec = [InputSpec(ndim = 3)]\n","    assert len(input_shape) == 3\n","    self.W = self.add_weight(shape = (input_shape[2], 1), name = \"{}_W\".format(self.name), initializer = self.init)\n","    self._trainable_weights = [self.W]\n","    super(AttentionWeightedAverage, self).build(input_shape)\n","  \n","  def call(self, x, mask = None):\n","    # computes a probability distribution over the timesteps\n","    # uses \"max trick\" for numerical stability\n","    # reshape is done to avoid issue with Tensorflow and 1-dimensional weights\n","    logits = K.dot(x, self.W)\n","    x_shape = K.shape(x)\n","    logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n","    ai = K.exp(logits - K.max(logits, axis = -1, keepdims = True))\n","    # mask timesteps have zero weight\n","    if mask is not None:\n","      mask = K.cast(mask, K.floatx())\n","      ai = ai * mask\n","    attn_weights = ai / (K.sum(ai, axis = 1, keepdims = True) + K.epsilon())\n","    weighted_input = x * K.expand_dims(attn_weights)\n","    result = K.sum(weighted_input, axis = 1)\n","    if self.return_attention:\n","      return [result, attn_weights]\n","    else:\n","      return result\n","  \n","  def get_output_shape_for(self, input_shape):\n","    return self.compute_output_shape(input_shape)\n","  \n","  def compute_output_shape(self, input_shape):\n","    output_len = input_shape[2]\n","    if self.return_attention:\n","      return [(input_shape[0], output_len), (input_shape[0], input_shape[1])]\n","    else:\n","      return (input_shape[0], output_len)\n","  \n","  def compute_mask(self, input, input_mask = None):\n","    if isinstance(input_mask, list):\n","      return [None] * len(input_mask)\n","    else:\n","      return [None]"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"OjyBP7IVmUr7","executionInfo":{"status":"error","timestamp":1621588512629,"user_tz":-480,"elapsed":68422,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}},"outputId":"a702d331-4857-4f01-8614-3b68694b4c45"},"source":["textgen = TextGenerator(vocab_path = \"TextGenerator_vocab.json\")\n","model = textgen.build_textgen_model(dropout = 0.2)\n","textgen.train_textgen(gen = gen, gen_val = gen_val, dropout = 0.2, num_epochs = 5, gen_epochs = 1, save_epochs = 2, verbose = 1, context = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1/5\n","   6/4839 [..............................] - ETA: 9:57:53 - loss: 9.3008 - context_output_loss: 9.2809 - output_loss: 9.3808"],"name":"stdout"},{"output_type":"stream","text":["ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-6-86d570b4855d>\", line 3, in <module>\n","    textgen.train_textgen(gen = gen, gen_val = gen_val, dropout = 0.2, num_epochs = 5, gen_epochs = 1, save_epochs = 2, verbose = 1, context = True)\n","  File \"<ipython-input-5-3c97d9c4add5>\", line 113, in train_textgen\n","    validation_steps = self.config[\"val_steps\"]\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\", line 1100, in fit\n","    tmp_logs = self.train_function(iterator)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 828, in __call__\n","    result = self._call(*args, **kwds)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\", line 855, in _call\n","    return self._stateless_fn(*args, **kwds)  # pylint: disable=not-callable\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 2943, in __call__\n","    filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 1919, in _call_flat\n","    ctx, args, cancellation_manager=cancellation_manager))\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\", line 560, in call\n","    ctx=ctx)\n","  File \"/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\", line 60, in quick_execute\n","    inputs, attrs, num_outputs)\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.7/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.7/inspect.py\", line 1502, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.7/inspect.py\", line 1460, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","  File \"/usr/lib/python3.7/inspect.py\", line 696, in getsourcefile\n","    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n","  File \"/usr/lib/python3.7/inspect.py\", line 742, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","  File \"/usr/lib/python3.7/posixpath.py\", line 395, in realpath\n","    path, ok = _joinrealpath(filename[:0], filename, {})\n","  File \"/usr/lib/python3.7/posixpath.py\", line 429, in _joinrealpath\n","    if not islink(newpath):\n","  File \"/usr/lib/python3.7/posixpath.py\", line 171, in islink\n","    st = os.lstat(path)\n","KeyboardInterrupt\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"xJFzu9E8nPVO"},"source":["from tqdm import trange\n","\n","class Predictor:\n","\n","  config = {\n","      \"rnn_layers\": 2,\n","      \"rnn_size\": 256,\n","      \"rnn_bidirectional\": True,\n","      \"max_length\": 100,\n","      \"max_words\": 100000,\n","      \"dim_embeddings\": 300,\n","      \"batch_size\": 128\n","  }\n","  default_config = config.copy()\n","\n","  def __init__(self, model, config_path = \"TextGenerator_config.json\", weights_path = None, vocab_path = None, name = \"TextGenerator\"):\n","    if config_path is not None:\n","      with open(config_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as json_file:\n","        self.config = json.load(json_file)\n","    if weights_path is None:\n","      weights_path = resource_filename(__name__, \"TextGenerator_weights.hdf5\")\n","    if vocab_path is None:\n","      vocab_path = resource_filename(__name__, \"TextGenerator_vocab.json\")\n","    if os.path.exists(vocab_path):\n","      with open(vocab_path, \"r\", encoding = \"utf8\", errors = \"ignore\") as json_file:\n","        self.vocab = json.load(json_file)\n","        self.num_classes = len(self.vocab) + 1\n","        self.indices_char = dict((self.vocab[c], c) for c in self.vocab)\n","    self.model = model\n","\n","  def generate(self, n = 1, return_as_list = False, prefix = None, temperature = [1.0, 0.5, 0.2, 0.2], max_gen_length = 300, interactive = False, top_n = 3, progress = True):\n","    gen_texts = []\n","    iterable = trange(n) if progress and n > 1 else range(n)\n","    for _ in iterable:\n","      gen_text, _ = self.generate_text(\n","          self.config[\"max_length\"],\n","          max_gen_length,\n","          top_n,\n","          temperature, \n","          self.meta_token,\n","          interactive, \n","          prefix\n","      )\n","      if not return_as_list:\n","        print(\"{}\\n\".format(gen_text))\n","      gen_texts.append(gen_text)\n","    if return_as_list:\n","      return gen_texts\n","  \n","  def generate_samples(self, n = 3, temperatures = [0.2, 0.5, 1.0], **kwargs):\n","    for temperature in temperatures:\n","      print(\"#\" * 20 + \"\\nTemperature: {}\\n\".formate(temperature) + \"#\" * 20)\n","      self.generate(n, temperature = temperature, progress = False, **kwargs)\n","      \n","  def generate_text(self, maxlen, top_n, temperature = 0.5, interactive = False, prefix = None, synthesize = False, stop_tokens = [\" \", \"\\n\"]):\n","    \"\"\" Generates and returns a single text. \"\"\"\n","    collapse_char = \"\"\n","    end = False\n","    text = [meta_token] + jieba.lcut(prefix) if prefix else [meta_token]\n","    next_char = \"\"\n","    if not isinstance(temperature, list):\n","      temperature = [temperature]\n","    while not end and len(text) < self.config[\"max_length\"]:\n","      encoded_text = self.prediction_encode_sequence(text[-maxlen:], self.vocab, maxlen)\n","      next_temperature = temperature[(len(text) - 1) % len(temperature)]\n","      if not interactive:\n","        # auto-generate text without user intervention\n","        next_index = self.generate_sample(self.model.predict(encoded_text, batch_size = 1)[0], next_temperature)\n","        next_char = self.indices_char[next_index]\n","        text += [next_char]\n","        if next_char == meta_token or len(text) >= self.config[\"max_length\"]:\n","          end = True\n","        gen_break = (next_char in stop_tokens or len(stop_tokens) == 0)\n","        if synthesize and gen_break:\n","          break\n","      else:\n","        # ask user what the next char/word should be\n","        options_index = self.generate_sample(self.model.predict(encoded_text, batch_size = 1)[0], next_temperature, interactive = interactive, top_n = top_n)\n","        options = [self.indices_char[idx] for idx in options_index]\n","        print(\"Controls:\\n\\ts: stop.\\tx: backspace.\\to: write your own.\")\n","        print(\"\\nOptions:\")\n","        for i, option in enumerate(options, 1):\n","          print(\"\\t{}: {}\".format(i, option))\n","        print(\"\\nProgress: {}\".format(collapse_char.join(text)[3:]))\n","        print(\"\\nYour choice?\")\n","        user_input = input(\"> \")\n","        try:\n","          user_input = int(user_input)\n","          next_char = options[user_input - 1]\n","          text += [next_char]\n","          if next_char == \"<s>\":\n","            end = True\n","        except ValueError:\n","          if user_input == \"s\":\n","            next_char = \"<s>\"\n","            text += [next_char]\n","          elif user_input == \"o\":\n","            other = input(\"> \")\n","            text += [other]\n","          elif user_input == \"x\":\n","            try:\n","              del text[-1]\n","            except IndexError:\n","              pass\n","          else:\n","            print(\"That\\'s not an option!\")\n","    # if not single text, remove the <s> meta_tokens\n","    text = text[1:]\n","    if meta_token in text:\n","      text.remove(meta_token)\n","    text_joined = collapse_char.join(text)\n","    return text_joined, end\n","\n","  def prediction_encode_sequence(self, text, vocab, maxlen):\n","    \"\"\" Encodes a text into the corresponding encoding for prediction with the model. \"\"\"\n","    encoded = np.array([vocab.get(x, 0) for x in text])\n","    return sequence.pad_sequences([encoded], maxlen = maxlen)\n","  \n","  def generate_sample(self, preds, temperature, interactive = False, top_n = 3):\n","    \"\"\" Samples predicted probabilities of the next character to allow for the network to show \"creativity\". \"\"\"\n","    preds = np.asarray(preds).astype(\"float64\")\n","    if temperature is None or temperature == 0.0:\n","      return np.argmax(preds)\n","    preds = np.log(preds + K.epsilon()) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    if not interactive:\n","      index = np.argmax(probas)\n","      # prevent function from being able to choose 0 (placeholder)\n","      # choose 2nd best index from preds\n","      if index == 0:\n","        index = np.argsort(preds)[-2]\n","    else:\n","      # return list of top N chars/words descending order, based on probability\n","      index = (-preds).argsort()[:top_n]\n","    return index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hjrQqS4opCIH","executionInfo":{"status":"ok","timestamp":1621908743029,"user_tz":-480,"elapsed":5371,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}}},"source":["textgen = TextGenerator(weights_path = \"NLG_2021_05_21/TextGenerator_weights_epoch_14.hdf5\", vocab_path = \"NLG_2021_05_21/TextGenerator_vocab.json\", config_path = \"NLG_2021_05_21/TextGenerator_config.json\")\n","# mod = textgen.build_textgen_model(dropout = 0.1)\n","# mod.load_weights(\"NLG_2021_05_21\\TextGenerator_weights_epoch_14.hdf5\")\n","\n","# pred = Predictor(mod)\n","# predictor.generate(n = 3, prefix = \"规划\", temperature = [0.65])"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gDp6-5DmRqss","executionInfo":{"status":"ok","timestamp":1621908789659,"user_tz":-480,"elapsed":8073,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}},"outputId":"3a3f6b7f-0315-468a-ba89-544976a4e9e1"},"source":["textgen.generate(n = 3, prefix = \"规划\", temperature = [0.5])"],"execution_count":27,"outputs":[{"output_type":"stream","text":[" 33%|███▎      | 1/3 [00:03<00:06,  3.48s/it]"],"name":"stderr"},{"output_type":"stream","text":["规划的配套设施不错 有一定的升值的空间和发展的潜力 周围的配套设施也比较的好 周围的环境也比较好 \n","\n"],"name":"stdout"},{"output_type":"stream","text":["\r 67%|██████▋   | 2/3 [00:05<00:03,  3.00s/it]"],"name":"stderr"},{"output_type":"stream","text":["规划的项目 现在规划的配套很给力 出门就是高速路 有些远\n","\n"],"name":"stdout"},{"output_type":"stream","text":["100%|██████████| 3/3 [00:07<00:00,  2.52s/it]"],"name":"stderr"},{"output_type":"stream","text":["规划的配套很完善 有沃尔玛 天虹 沃尔玛 华润万家 天虹 沃尔玛等 \n","\n"],"name":"stdout"},{"output_type":"stream","text":["\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CatCb63T70WE","executionInfo":{"status":"ok","timestamp":1621327488120,"user_tz":-480,"elapsed":36533,"user":{"displayName":"Ziye Zhou","photoUrl":"","userId":"15792268413502645603"}},"outputId":"3a822b2f-1346-4efa-f102-2b07271122f9"},"source":["predictor = Predictor(textgen.model)\n","predictor.generate(n = 1, prefix = \"规划\", temperature = [0.8], interactive = True)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 的\n","\t2: 很\n","\t3: 不错\n","\n","Progress: 规划\n","\n","Your choice?\n","> 3\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: ，\n","\t2: 的\n","\t3: 了\n","\n","Progress: 规划不错\n","\n","Your choice?\n","> 1\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 周边\n","\t2: 配套\n","\t3: 户型\n","\n","Progress: 规划不错，\n","\n","Your choice?\n","> 3\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 也\n","\t2: 方正\n","\t3: 不错\n","\n","Progress: 规划不错，户型\n","\n","Your choice?\n","> 2\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 实用\n","\t2: ，\n","\t3: 合理\n","\n","Progress: 规划不错，户型方正\n","\n","Your choice?\n","> 2\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 使用率\n","\t2: 性价比\n","\t3: 南北\n","\n","Progress: 规划不错，户型方正，\n","\n","Your choice?\n","> 3\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: 通透\n","\t2: 通\n","\t3: 合理\n","\n","Progress: 规划不错，户型方正，南北\n","\n","Your choice?\n","> 1\n","Controls:\n","\ts: stop.\tx: backspace.\to: write your own.\n","\n","Options:\n","\t1: ，\n","\t2: 。\n","\t3: <s>\n","\n","Progress: 规划不错，户型方正，南北通透\n","\n","Your choice?\n","> 3\n","规划不错，户型方正，南北通透\n","\n"],"name":"stdout"}]}]}